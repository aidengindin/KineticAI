{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import Generator, Optional, Dict\n",
    "from fitparse import FitFile, FitParseError\n",
    "from dataclasses import dataclass\n",
    "import traceback\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing data/fit/9697089315.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/13526142366.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/4501092610.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/7355956985.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/1295842048.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/12930284250.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/6447648957.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/7602898040.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 40\n",
      "Native field: Form Power = 21\n",
      "Native field: Air Power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/5149258619.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/7416863238.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/5606804154.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: RP_Power = None\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/5131044181.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/6086023702.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: Power = 0\n",
      "Native field: Form Power = 0\n",
      "Native field: Air Power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/3040006095.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/6479355289.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/10113976218.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/12524158420.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: Power = 96\n",
      "Native field: Form Power = 51\n",
      "Native field: Air Power = 1\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/13447183676.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/6821139086.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "Native field: Form Power = 0\n",
      "Native field: Air Power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/6119309936.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/3016570651.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/7767710055.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/12482923568.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/5100951957.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/13308573838.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: Power = 0\n",
      "Native field: Form Power = 0\n",
      "Native field: Air Power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/6659359802.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/8465166075.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/12909425317.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 229\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/7828436076.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/12350761483.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/8387074194.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/12890715549.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/12886971442.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/5152747954.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/12949131063.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/8794128380.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/12317009435.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/13833609296.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/5706961013.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/9073581344.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 186\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/4028790446.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/11671878538.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/13703611065.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/6812381562.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "Native field: Form Power = 0\n",
      "Native field: Air Power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/10336513290.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/3001475251.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/7491434833.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/4327744552.fit:\n",
      "\n",
      "Record fields:\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/6661274212.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: Power = 0\n",
      "Native field: Form Power = 0\n",
      "Native field: Air Power = 0\n",
      "\n",
      "Developer fields:\n",
      "\n",
      "Analyzing data/fit/6947696460.fit:\n",
      "\n",
      "Record fields:\n",
      "Native field: power = 0\n",
      "Native field: Form Power = 0\n",
      "Native field: Air Power = 0\n",
      "\n",
      "Developer fields:\n"
     ]
    }
   ],
   "source": [
    "def analyze_power_fields(fit_path: str):\n",
    "    \"\"\"Debug function to examine power-related fields in a FIT file\"\"\"\n",
    "    try:\n",
    "        fit_file = FitFile(fit_path)\n",
    "        \n",
    "        # Look at all record fields to find power-related ones\n",
    "        for record in fit_file.get_messages('record'):\n",
    "            fields = record.fields\n",
    "            dev_fields = record.dev_fields if hasattr(record, 'dev_fields') else []\n",
    " \n",
    "            print(f\"\\nRecord fields:\")\n",
    "            for field in fields:\n",
    "                if 'power' in field.name.lower():\n",
    "                    print(f\"Native field: {field.name} = {record.get_value(field.name)}\")\n",
    "                    \n",
    "            print(f\"\\nDeveloper fields:\")\n",
    "            for field in dev_fields:\n",
    "                if 'power' in field.name.lower():\n",
    "                    print(f\"Developer field: {field.name} = {record.get_value(field.name)}\")\n",
    "            \n",
    "            break  # Just look at first record\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {fit_path}: {e}\")\n",
    "\n",
    "# Test with a few files\n",
    "import glob\n",
    "for fit_file in glob.glob('data/fit/*.fit')[:50]:\n",
    "    print(f\"\\nAnalyzing {fit_file}:\")\n",
    "    analyze_power_fields(fit_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing data/fit/9697089315.fit:\n",
      "\n",
      "Activity messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2023-05-09 11:11:51\n",
      "total_timer_time = 2013.94\n",
      "local_timestamp = 2023-05-09 07:11:51\n",
      "num_sessions = 1\n",
      "type = manual\n",
      "event = activity\n",
      "event_type = stop\n",
      "event_group = None\n",
      "unknown_7 = None\n",
      "\n",
      "Session messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2023-05-09 11:11:51\n",
      "start_time = 2023-05-09 10:38:09\n",
      "start_position_lat = None\n",
      "start_position_long = None\n",
      "total_elapsed_time = 2013.94\n",
      "total_timer_time = 2013.94\n",
      "time_standing = None\n",
      "avg_stance_time_percent = None\n",
      "avg_stance_time = None\n",
      "avg_stance_time_balance = None\n",
      "\n",
      "Event messages (type=start):\n",
      "\n",
      "Fields:\n",
      "timestamp = 2023-05-09 10:38:09\n",
      "timer_trigger = manual\n",
      "unknown_15 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Record fields:\n",
      "timestamp = 2023-05-09 10:38:09\n",
      "distance = 0.0\n",
      "heart_rate = 78\n",
      "unknown_134 = None\n",
      "\n",
      "Analyzing data/fit/13526142366.fit:\n",
      "\n",
      "Activity messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2024-10-18 16:53:38\n",
      "total_timer_time = 1026.013\n",
      "local_timestamp = 2024-10-18 12:53:38\n",
      "unknown_8 = None\n",
      "num_sessions = 1\n",
      "type = manual\n",
      "event = activity\n",
      "event_type = stop\n",
      "event_group = None\n",
      "unknown_7 = None\n",
      "\n",
      "Session messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2024-10-18 16:53:38\n",
      "start_time = 2024-10-18 16:53:38\n",
      "start_position_lat = 510848769\n",
      "start_position_long = -853115684\n",
      "total_elapsed_time = 1026.013\n",
      "total_timer_time = 1026.013\n",
      "total_moving_time = None\n",
      "time_standing = None\n",
      "avg_stance_time_percent = None\n",
      "avg_stance_time = None\n",
      "avg_stance_time_balance = None\n",
      "\n",
      "Event messages (type=start):\n",
      "\n",
      "Fields:\n",
      "timestamp = 2024-10-18 16:53:38\n",
      "timer_trigger = manual\n",
      "unknown_15 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Record fields:\n",
      "timestamp = 2024-10-18 16:53:38\n",
      "position_lat = 510848769\n",
      "position_long = -853115684\n",
      "distance = 0.0\n",
      "enhanced_speed = 0.0\n",
      "enhanced_altitude = 66.39999999999998\n",
      "unknown_87 = None\n",
      "heart_rate = 81\n",
      "cadence = 0\n",
      "fractional_cadence = 0.0\n",
      "unknown_107 = None\n",
      "unknown_134 = None\n",
      "unknown_135 = None\n",
      "unknown_136 = None\n",
      "unknown_143 = None\n",
      "\n",
      "Analyzing data/fit/4501092610.fit:\n",
      "\n",
      "Activity messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:41:37\n",
      "total_timer_time = 10141.467\n",
      "local_timestamp = 2020-10-17 11:41:37\n",
      "num_sessions = 1\n",
      "type = manual\n",
      "event = activity\n",
      "event_type = stop\n",
      "event_group = None\n",
      "unknown_7 = None\n",
      "\n",
      "Session messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:41:37\n",
      "start_time = 2020-10-17 12:38:58\n",
      "start_position_lat = 495122656\n",
      "start_position_long = -973571951\n",
      "total_elapsed_time = 10918.922\n",
      "total_timer_time = 10141.467\n",
      "time_standing = None\n",
      "avg_stance_time_percent = None\n",
      "avg_stance_time = None\n",
      "avg_stance_time_balance = None\n",
      "\n",
      "Event messages (type=start):\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 12:38:58\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 12:41:00\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 12:54:16\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 12:54:34\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 12:55:00\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 12:55:12\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:03:18\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:04:24\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:09:06\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:09:32\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:10:25\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:11:48\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:11:53\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:15:24\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:29:02\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:29:09\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:30:11\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:32:32\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:41:35\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:50:57\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:55:57\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:57:35\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 13:58:10\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 14:18:32\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 14:35:34\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 14:48:32\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 14:53:10\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 14:54:10\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:09:50\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:15:20\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:22:37\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:22:44\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:24:55\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:27:54\n",
      "data = 0\n",
      "event = off_course\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:32:51\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Fields:\n",
      "timestamp = 2020-10-17 15:33:50\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "unknown_19 = None\n",
      "unknown_20 = None\n",
      "\n",
      "Record fields:\n",
      "timestamp = 2020-10-17 12:38:58\n",
      "position_lat = 495122656\n",
      "position_long = -973571951\n",
      "distance = 0.0\n",
      "enhanced_speed = 0.0\n",
      "enhanced_altitude = 217.20000000000005\n",
      "unknown_87 = None\n",
      "heart_rate = 89\n",
      "cadence = 0\n",
      "temperature = 30\n",
      "fractional_cadence = 0.0\n",
      "\n",
      "Analyzing data/fit/7355956985.fit:\n",
      "\n",
      "Activity messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 21:56:52\n",
      "total_timer_time = 6386.423\n",
      "local_timestamp = 2022-03-31 17:56:52\n",
      "num_sessions = 1\n",
      "type = manual\n",
      "event = activity\n",
      "event_type = stop\n",
      "event_group = None\n",
      "\n",
      "Session messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 21:56:52\n",
      "start_time = 2022-03-31 19:59:39\n",
      "start_position_lat = 495125271\n",
      "start_position_long = -973572451\n",
      "total_elapsed_time = 7019.416\n",
      "total_timer_time = 6386.423\n",
      "time_standing = None\n",
      "\n",
      "Event messages (type=start):\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 19:59:39\n",
      "timer_trigger = manual\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 19:59:43\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 20:06:19\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 20:08:50\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 20:13:42\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 20:18:25\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 20:28:56\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 20:33:09\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 20:35:12\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 20:44:05\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 20:59:51\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 21:21:45\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 21:28:04\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 21:31:09\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 21:39:15\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 21:45:36\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 21:48:53\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2022-03-31 21:56:21\n",
      "timer_trigger = auto\n",
      "unknown_17 = None\n",
      "unknown_18 = None\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Record fields:\n",
      "timestamp = 2022-03-31 19:59:39\n",
      "position_lat = 495125271\n",
      "position_long = -973572451\n",
      "distance = 0.0\n",
      "enhanced_altitude = 226.79999999999995\n",
      "altitude = 226.79999999999995\n",
      "enhanced_speed = 0.0\n",
      "speed = 0.0\n",
      "power = 0\n",
      "unknown_61 = None\n",
      "unknown_66 = None\n",
      "heart_rate = 84\n",
      "temperature = 21\n",
      "\n",
      "Analyzing data/fit/1295842048.fit:\n",
      "\n",
      "Activity messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2017-09-18 16:02:18\n",
      "total_timer_time = 591.741\n",
      "local_timestamp = 2017-09-18 09:02:18\n",
      "num_sessions = 1\n",
      "type = manual\n",
      "event = activity\n",
      "event_type = stop\n",
      "event_group = None\n",
      "\n",
      "Session messages:\n",
      "\n",
      "Fields:\n",
      "timestamp = 2017-09-18 16:02:18\n",
      "start_time = 2017-09-18 15:51:54\n",
      "start_position_lat = 567305999\n",
      "start_position_long = -1458599477\n",
      "total_elapsed_time = 610.129\n",
      "total_timer_time = 591.741\n",
      "avg_stance_time_percent = None\n",
      "avg_stance_time = None\n",
      "\n",
      "Event messages (type=start):\n",
      "\n",
      "Fields:\n",
      "timestamp = 2017-09-18 15:51:54\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Fields:\n",
      "timestamp = 2017-09-18 15:54:05\n",
      "timer_trigger = manual\n",
      "event = timer\n",
      "event_type = start\n",
      "event_group = 0\n",
      "\n",
      "Record fields:\n",
      "timestamp = 2017-09-18 15:51:54\n",
      "position_lat = 567305999\n",
      "position_long = -1458599477\n",
      "distance = 0.31\n",
      "enhanced_altitude = 1.0\n",
      "altitude = 1.0\n",
      "enhanced_speed = 1.568\n",
      "speed = 1.568\n",
      "cadence = 0\n",
      "fractional_cadence = 0.0\n"
     ]
    }
   ],
   "source": [
    "def debug_activity_messages(fit_path: str):\n",
    "    \"\"\"Debug function to examine activity-related messages in a FIT file\"\"\"\n",
    "    try:\n",
    "        fit_file = FitFile(fit_path)\n",
    "        \n",
    "        print(\"\\nActivity messages:\")\n",
    "        for message in fit_file.get_messages('activity'):\n",
    "            print(\"\\nFields:\")\n",
    "            for field in message.fields:\n",
    "                print(f\"{field.name} = {message.get_value(field.name)}\")\n",
    "                \n",
    "        print(\"\\nSession messages:\")\n",
    "        for message in fit_file.get_messages('session'):\n",
    "            print(\"\\nFields:\")\n",
    "            for field in message.fields:\n",
    "                if 'time' in field.name.lower() or 'start' in field.name.lower():\n",
    "                    print(f\"{field.name} = {message.get_value(field.name)}\")\n",
    "        \n",
    "        print(\"\\nEvent messages (type=start):\")\n",
    "        for message in fit_file.get_messages('event'):\n",
    "            if message.get_value('event_type') == 'start':\n",
    "                print(\"\\nFields:\")\n",
    "                for field in message.fields:\n",
    "                    print(f\"{field.name} = {message.get_value(field.name)}\")\n",
    "\n",
    "        # Look at first few records\n",
    "        for i, record in enumerate(fit_file.get_messages('record')):\n",
    "            if i == 0:  # Just look at first record\n",
    "                print(\"\\nRecord fields:\")\n",
    "                for field in record.fields:\n",
    "                    print(f\"{field.name} = {record.get_value(field.name)}\")\n",
    "                    \n",
    "                # Also look at raw fields if available\n",
    "                if hasattr(record, 'raw_values'):\n",
    "                    print(\"\\nRaw values:\")\n",
    "                    for name, value in record.raw_values.items():\n",
    "                        print(f\"{name} = {value}\")\n",
    "            break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {fit_path}: {e}\")\n",
    "\n",
    "# Test with a few files\n",
    "import glob\n",
    "for fit_file in glob.glob('data/fit/*.fit')[:5]:\n",
    "    print(f\"\\nAnalyzing {fit_file}:\")\n",
    "    debug_activity_messages(fit_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_fit_files(fit_directory: str, output_path: str = 'data/processed/activities.csv') -> None:\n",
    "    \"\"\"\n",
    "    Process all FIT files in directory and save relevant features to CSV incrementally.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all fit files\n",
    "    fit_files = [f for f in Path(fit_directory).glob('*.fit')]\n",
    "    print(f\"Found {len(fit_files)} FIT files\")\n",
    "    \n",
    "    # Initialize counters\n",
    "    skipped_files = 0\n",
    "    processed_files = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    # Define CSV headers\n",
    "    headers = ['activity_id', 'timestamp', 'sport', 'heart_rate', 'speed', 'gap',\n",
    "              'cadence', 'power_ind', 'power', 'latitude', 'longitude',\n",
    "              'altitude', 'time_into_activity']\n",
    "    \n",
    "    # Create/open CSV file with headers\n",
    "    with open(output_path, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Process each file\n",
    "        for fit_path in tqdm(fit_files):\n",
    "            try:\n",
    "                fit_path_str = str(fit_path)\n",
    "                fit_file = FitFile(fit_path_str)\n",
    "                session_msg = next(fit_file.get_messages('session'))\n",
    "                if session_msg.get_value('sport') not in ['running', 'cycling']:\n",
    "                    continue\n",
    "                if session_msg.get_value('sub_sport') in ['indoor_running', 'indoor_cycling', 'virtual_activity']:\n",
    "                    continue\n",
    "                records = list(fit_file.get_messages('record'))\n",
    "                has_position_data = any(r.get_value('position_lat') is not None for r in records[:60])\n",
    "                if not has_position_data:\n",
    "                    continue\n",
    "                \n",
    "                # If we get here, process the activity records\n",
    "                activity_id = fit_path.stem  # filename without extension\n",
    "                \n",
    "                activity_records = []  # Buffer for this activity's records\n",
    "\n",
    "                activity_start_time = None\n",
    "                for message in fit_file.get_messages('session'):\n",
    "                    activity_start_time = message.get_value('start_time')\n",
    "                    break\n",
    "                if not activity_start_time:\n",
    "                    print(f'Warning: no activity start time found for {fit_path}')\n",
    "                    continue\n",
    "\n",
    "                sport = session_msg.get_value('sport')\n",
    "                has_stryd = any('air power' in field.name.lower() for field in session_msg.fields)\n",
    "                has_power = any('power' in field.name.lower() for field in session_msg.fields)\n",
    "\n",
    "                power_ind = 0\n",
    "\n",
    "                if sport == 'running':\n",
    "                    power_ind = 1 if has_stryd else 0\n",
    "                elif sport == 'cycling':\n",
    "                    power_ind = 1 if has_power else 0\n",
    "                \n",
    "                for record in fit_file.get_messages('record'):\n",
    "                    try:\n",
    "                        power = 0\n",
    "                        if sport == 'running' and has_stryd:\n",
    "                                power = record.get_value('power')\n",
    "                        elif sport == 'cycling' and has_power:\n",
    "                            power = record.get_value('power')\n",
    "\n",
    "                        speed = record.get_value('speed') or record.get_value('enhanced_speed')\n",
    "                        altitude = record.get_value('altitude') or record.get_value('enhanced_altitude') or 0\n",
    "\n",
    "                        # Extract relevant features\n",
    "                        record_dict = {\n",
    "                            'activity_id': activity_id,\n",
    "                            'timestamp': record.get_value('timestamp'),\n",
    "                            'sport': sport,\n",
    "                            'heart_rate': record.get_value('heart_rate'),\n",
    "                            'speed': speed,\n",
    "                            'gap': 0,\n",
    "                            'cadence': record.get_value('cadence'),\n",
    "                            'power_ind': power_ind,\n",
    "                            'power': power,\n",
    "                            'latitude': record.get_value('position_lat'),\n",
    "                            'longitude': record.get_value('position_long'),\n",
    "                            'altitude': altitude,\n",
    "                            'time_into_activity': int((record.get_value('timestamp') - activity_start_time).total_seconds())\n",
    "                        }\n",
    "                        \n",
    "                        # Only add records that have at least heart rate and speed\n",
    "                        if record_dict['heart_rate'] is not None and speed is not None:\n",
    "                            activity_records.append(record_dict)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing record in {fit_path}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # Write all records for this activity\n",
    "                writer.writerows(activity_records)\n",
    "                \n",
    "                total_records += len(activity_records)\n",
    "                processed_files += 1\n",
    "                \n",
    "                # Flush periodically to ensure data is written to disk\n",
    "                if processed_files % 100 == 0:\n",
    "                    f.flush()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {fit_path}: {str(e)}\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Processed {processed_files} files\")\n",
    "    print(f\"Skipped {skipped_files} files\")\n",
    "    print(f\"Total records: {total_records}\")\n",
    "    print(f\"\\nSaved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3090 FIT files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3090/3090 [20:43<00:00,  2.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Processed 1261 files\n",
      "Skipped 0 files\n",
      "Total records: 3769174\n",
      "\n",
      "Saved to ./data/records.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_fit_files('./data/fit/', output_path='./data/records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coordinates(input_path: str, output_path: str):\n",
    "    \"\"\"Convert coordinates from semicircles to degrees in place.\"\"\"\n",
    "    \n",
    "    print(\"Converting coordinates from semicircles to degrees...\")\n",
    "    \n",
    "    # Conversion constant\n",
    "    SEMICIRCLES_TO_DEGREES = 180.0 / (2**31)\n",
    "    \n",
    "    # Read CSV in chunks to handle large files\n",
    "    chunk_size = 100000\n",
    "    first_chunk = True\n",
    "    \n",
    "    for chunk in pd.read_csv(input_path, chunksize=chunk_size):\n",
    "        # Convert coordinates\n",
    "        chunk['latitude'] = chunk['latitude'] * SEMICIRCLES_TO_DEGREES\n",
    "        chunk['longitude'] = chunk['longitude'] * SEMICIRCLES_TO_DEGREES\n",
    "        \n",
    "        # Write to file\n",
    "        if first_chunk:\n",
    "            chunk.to_csv(output_path, index=False, mode='w')\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            chunk.to_csv(output_path, index=False, mode='a', header=False)\n",
    "    \n",
    "    print(\"Conversion complete!\")\n",
    "    \n",
    "    # Verify a few values\n",
    "    df_sample = pd.read_csv(output_path, nrows=5)\n",
    "    print(\"\\nSample of converted coordinates:\")\n",
    "    print(df_sample[['latitude', 'longitude']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting coordinates from semicircles to degrees...\n",
      "Conversion complete!\n",
      "\n",
      "Sample of converted coordinates:\n",
      "    latitude  longitude\n",
      "0  41.500702 -81.603858\n",
      "1  41.500709 -81.603847\n",
      "2  41.500722 -81.603832\n",
      "3  41.500736 -81.603809\n",
      "4  41.500753 -81.603776\n"
     ]
    }
   ],
   "source": [
    "convert_coordinates(\n",
    "    input_path='./data/records.csv',\n",
    "    output_path='./data/records_converted.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_locations(input_path: str):\n",
    "    \"\"\"Sample one location per 30 minutes per activity and get city-level location data.\"\"\"\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sample one location per 30 minutes per activity\n",
    "    sampled_locations = []\n",
    "    for activity_id, group in df.groupby('activity_id'):\n",
    "        # Get location at each 30-minute mark\n",
    "        activity_samples = group.groupby(pd.Grouper(key='timestamp', freq='60min')).agg({\n",
    "            'latitude': 'first',\n",
    "            'longitude': 'first'\n",
    "        }).dropna()\n",
    "        sampled_locations.append(activity_samples)\n",
    "    \n",
    "    locations_df = pd.concat(sampled_locations)\n",
    "    print(f\"Sampled {len(locations_df)} locations from {len(df['activity_id'].unique())} activities\")\n",
    "    \n",
    "    # Get unique lat/lon pairs to minimize API calls\n",
    "    unique_locations = locations_df.drop_duplicates(['latitude', 'longitude'])\n",
    "    print(f\"Found {len(unique_locations)} unique locations\")\n",
    "    \n",
    "    # Query Nominatim for each unique location\n",
    "    location_info = {}\n",
    "    for idx, row in tqdm(unique_locations.iterrows(), total=len(unique_locations)):\n",
    "        try:\n",
    "            url = \"https://nominatim.openstreetmap.org/reverse\"\n",
    "            params = {\n",
    "                'lat': row['latitude'],\n",
    "                'lon': row['longitude'],\n",
    "                'format': 'jsonv2',\n",
    "                'zoom': 10  # City level\n",
    "            }\n",
    "            \n",
    "            response = requests.get(\n",
    "                url, \n",
    "                params=params,\n",
    "                headers={'User-Agent': 'KineticAI/1.0'}  # Required by Nominatim\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Get city or nearest populated place\n",
    "            city = (\n",
    "                data['address'].get('city') or \n",
    "                data['address'].get('town') or \n",
    "                data['address'].get('village') or\n",
    "                data['address'].get('suburb')\n",
    "            )\n",
    "            \n",
    "            state = (\n",
    "                data['address'].get('state') or\n",
    "                data['address'].get('state_district')\n",
    "            )\n",
    "            \n",
    "            country = data['address'].get('country')\n",
    "            \n",
    "            location_key = f\"{city}, {state}, {country}\" if state else f\"{city}, {country}\"\n",
    "            location_info[(row['latitude'], row['longitude'])] = location_key\n",
    "            \n",
    "            # Respect rate limit\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting location info for {row['latitude']}, {row['longitude']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Count activities per location\n",
    "    location_counts = Counter()\n",
    "    for idx, row in locations_df.iterrows():\n",
    "        loc_key = location_info.get((row['latitude'], row['longitude']))\n",
    "        if loc_key:\n",
    "            location_counts[loc_key] += 1\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nActivity distribution by location:\")\n",
    "    for location, count in sorted(location_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{location}: {count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Sampled 902 locations from 387 activities\n",
      "Found 891 unique locations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 217/891 [04:52<12:18,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting location info for 0.0, 0.0: 'address'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 891/891 [19:58<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Activity distribution by location:\n",
      "Manchester, New Hampshire, United States: 292 records\n",
      "Cleveland, Ohio, United States: 141 records\n",
      "None, Virginia, United States: 37 records\n",
      "City of Yonkers, New York, United States: 35 records\n",
      "Cleveland Heights, Ohio, United States: 34 records\n",
      "Goffstown, New Hampshire, United States: 24 records\n",
      "Ludlow, Vermont, United States: 22 records\n",
      "Bedford, New Hampshire, United States: 20 records\n",
      "Hooksett, New Hampshire, United States: 19 records\n",
      "Town of Bedford, New York, United States: 15 records\n",
      "Shaker Heights, Ohio, United States: 15 records\n",
      "Ann Arbor, Michigan, United States: 14 records\n",
      "City of New York, New York, United States: 13 records\n",
      "None, Hawaii, United States: 13 records\n",
      "Pepper Pike, Ohio, United States: 12 records\n",
      "Merrimack, New Hampshire, United States: 10 records\n",
      "Dunbarton, New Hampshire, United States: 10 records\n",
      "None, New York, United States: 9 records\n",
      "Gates Mills, Ohio, United States: 9 records\n",
      "None, California, United States: 9 records\n",
      "Beachwood, Ohio, United States: 8 records\n",
      "New Boston, New Hampshire, United States: 8 records\n",
      "Plymouth, Vermont, United States: 7 records\n",
      "Seattle, Washington, United States: 7 records\n",
      "Amherst, New Hampshire, United States: 4 records\n",
      "Hunting Valley, Ohio, United States: 4 records\n",
      "Woodstock, Vermont, United States: 3 records\n",
      "Weathersfield, Vermont, United States: 3 records\n",
      "Londonderry, New Hampshire, United States: 3 records\n",
      "Town of Stony Point, New York, United States: 3 records\n",
      "Weare, New Hampshire, United States: 3 records\n",
      "Bratenahl, Ohio, United States: 2 records\n",
      "Bridgewater, Vermont, United States: 2 records\n",
      "Auburn, New Hampshire, United States: 2 records\n",
      "South Euclid, Ohio, United States: 2 records\n",
      "Willoughby Hills, Ohio, United States: 2 records\n",
      "Chagrin Falls, Ohio, United States: 2 records\n",
      "Rainier, Washington, United States: 2 records\n",
      "None, Ohio, United States: 2 records\n",
      "Village of Dobbs Ferry, New York, United States: 2 records\n",
      "San Francisco, California, United States: 2 records\n",
      "Mont Vernon, New Hampshire, United States: 2 records\n",
      "Village of Hastings-on-Hudson, New York, United States: 2 records\n",
      "Nashua, New Hampshire, United States: 2 records\n",
      "Mount Holly, Vermont, United States: 2 records\n",
      "Ogunquit, Maine, United States: 2 records\n",
      "Wells, Maine, United States: 2 records\n",
      "Town of Pound Ridge, New York, United States: 2 records\n",
      "Lincoln, New Hampshire, United States: 2 records\n",
      "None, New Hampshire, United States: 2 records\n",
      "Village of Tarrytown, New York, United States: 1 records\n",
      "Village of Rye Brook, New York, United States: 1 records\n",
      "City of New Rochelle, New York, United States: 1 records\n",
      "Edinburg, Virginia, United States: 1 records\n",
      "Mount Jackson, Virginia, United States: 1 records\n",
      "Lebanon, New Hampshire, United States: 1 records\n",
      "Plainfield, New Hampshire, United States: 1 records\n",
      "Charlestown, New Hampshire, United States: 1 records\n",
      "Chester, Vermont, United States: 1 records\n",
      "Cavendish, Vermont, United States: 1 records\n",
      "Town of Mamaroneck, New York, United States: 1 records\n",
      "Pepperell, Massachusetts, United States: 1 records\n",
      "Orange, Ohio, United States: 1 records\n",
      "Valley View, Ohio, United States: 1 records\n",
      "Euclid, Ohio, United States: 1 records\n",
      "Mayfield Heights, Ohio, United States: 1 records\n",
      "East Cleveland, Ohio, United States: 1 records\n",
      "Town of Haverstraw, New York, United States: 1 records\n",
      "Mayfield, Ohio, United States: 1 records\n",
      "Moreland Hills, Ohio, United States: 1 records\n",
      "Waite Hill, Ohio, United States: 1 records\n",
      "Walton Hills, Ohio, United States: 1 records\n",
      "Brecksville, Ohio, United States: 1 records\n",
      "Middleburg Heights, Ohio, United States: 1 records\n",
      "Bradford, New Hampshire, United States: 1 records\n",
      "Newport, New Hampshire, United States: 1 records\n",
      "Claremont, New Hampshire, United States: 1 records\n",
      "Pacifica, California, United States: 1 records\n",
      "Alpine, New Jersey, United States: 1 records\n",
      "Hollis, New Hampshire, United States: 1 records\n",
      "Reading, Vermont, United States: 1 records\n",
      "Derry, New Hampshire, United States: 1 records\n",
      "Hampstead, New Hampshire, United States: 1 records\n",
      "Chester, New Hampshire, United States: 1 records\n",
      "Hudson, New Hampshire, United States: 1 records\n",
      "Windham, New Hampshire, United States: 1 records\n",
      "Deering, New Hampshire, United States: 1 records\n",
      "Henniker, New Hampshire, United States: 1 records\n",
      "Concord, New Hampshire, United States: 1 records\n",
      "Village of Irvington, New York, United States: 1 records\n",
      "Candia, New Hampshire, United States: 1 records\n",
      "Killington, Vermont, United States: 1 records\n",
      "None, Vermont, United States: 1 records\n",
      "Barnard, Vermont, United States: 1 records\n",
      "Allenstown, New Hampshire, United States: 1 records\n",
      "Woodstock, New Hampshire, United States: 1 records\n"
     ]
    }
   ],
   "source": [
    "verify_locations('./data/records_converted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_weather_data(input_path: str, output_path: str):\n",
    "    \"\"\"Add weather data to activity records, minimizing API calls by sampling every 30 minutes per activity.\"\"\"\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = pd.read_csv(input_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Filter out (0,0) coordinates\n",
    "    df = df[~((df['latitude'] == 0) & (df['longitude'] == 0))]\n",
    "    df = df.sort_values(['activity_id', 'timestamp'])\n",
    "    \n",
    "    # Create weather timestamp column (rounded to nearest 30 minutes)\n",
    "    df['weather_timestamp'] = df['timestamp'].dt.floor('30min')\n",
    "    df['lat_rounded'] = df['latitude'].round(2)\n",
    "    df['lon_rounded'] = df['longitude'].round(2)\n",
    "    \n",
    "    # For each activity, sample location every 30 minutes\n",
    "    weather_points = df.groupby(['activity_id', 'weather_timestamp']).agg({\n",
    "        'lat_rounded': 'first',\n",
    "        'lon_rounded': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Get unique location-time combinations\n",
    "    unique_queries = weather_points.drop_duplicates(['lat_rounded', 'lon_rounded', 'weather_timestamp'])\n",
    "    print(f\"Found {len(unique_queries)} unique location-time combinations\")\n",
    "    \n",
    "    # Initialize weather data storage\n",
    "    weather_cache = {}\n",
    "    \n",
    "    # Weather variables we want\n",
    "    hourly_params = [\n",
    "        'temperature_2m',\n",
    "        'relative_humidity_2m',\n",
    "        'dew_point_2m',\n",
    "        'wind_speed_10m',\n",
    "        'wind_direction_10m',\n",
    "        'precipitation',\n",
    "        'cloud_cover',\n",
    "        'surface_pressure'\n",
    "    ]\n",
    "    \n",
    "    print(\"Fetching weather data...\")\n",
    "    # Process in batches to avoid rate limits\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(unique_queries), batch_size)):\n",
    "        batch = unique_queries.iloc[i:i+batch_size]\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            cache_key = (row['lat_rounded'], row['lon_rounded'], row['weather_timestamp'])\n",
    "            \n",
    "            if cache_key in weather_cache:\n",
    "                continue\n",
    "                \n",
    "            # Get weather data for this location and hour\n",
    "            start_date = row['weather_timestamp'].strftime('%Y-%m-%d')\n",
    "            end_date = (row['weather_timestamp'] + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "            \n",
    "            url = 'https://archive-api.open-meteo.com/v1/archive'\n",
    "            params = {\n",
    "                'latitude': row['lat_rounded'],\n",
    "                'longitude': row['lon_rounded'],\n",
    "                'start_date': start_date,\n",
    "                'end_date': end_date,\n",
    "                'hourly': ','.join(hourly_params)\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                # Find the matching hour in the response\n",
    "                target_hour = row['weather_timestamp']\n",
    "                target_hour_str = target_hour.strftime('%Y-%m-%dT%H:00')\n",
    "                \n",
    "                try:\n",
    "                    hour_index = data['hourly']['time'].index(target_hour_str)\n",
    "                    \n",
    "                    # Store weather data for this location-hour\n",
    "                    weather_cache[cache_key] = {\n",
    "                        param: data['hourly'][param][hour_index]\n",
    "                        for param in hourly_params\n",
    "                    }\n",
    "                except ValueError:\n",
    "                    print(f\"Could not find hour {target_hour_str} in weather data\")\n",
    "                    continue\n",
    "                \n",
    "                # Respect rate limits\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching weather data for {row['lat_rounded']}, {row['lon_rounded']}, {row['weather_timestamp']}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    print(\"Adding weather data to records...\")\n",
    "    # Create a new dataframe with weather data\n",
    "    weather_data = []\n",
    "    \n",
    "    # Group by activity to ensure we use the right weather data within each activity\n",
    "    for activity_id, group in df.groupby('activity_id'):\n",
    "        # Get weather points for this activity\n",
    "        activity_weather = weather_points[weather_points['activity_id'] == activity_id]\n",
    "        \n",
    "        # For each record in the activity\n",
    "        for _, record in group.iterrows():\n",
    "            # Find the closest weather timestamp for this record\n",
    "            weather_matches = activity_weather[\n",
    "                (activity_weather['weather_timestamp'] >= record['weather_timestamp'] - pd.Timedelta(minutes=30)) &\n",
    "                (activity_weather['weather_timestamp'] <= record['weather_timestamp'] + pd.Timedelta(minutes=30))\n",
    "            ]\n",
    "            \n",
    "            if len(weather_matches) > 0:\n",
    "                # Use the closest weather point in time\n",
    "                closest_weather = weather_matches.iloc[0]\n",
    "                cache_key = (\n",
    "                    closest_weather['lat_rounded'],\n",
    "                    closest_weather['lon_rounded'],\n",
    "                    closest_weather['weather_timestamp']\n",
    "                )\n",
    "                \n",
    "                if cache_key in weather_cache:\n",
    "                    weather_row = weather_cache[cache_key].copy()\n",
    "                    weather_row['activity_id'] = record['activity_id']\n",
    "                    weather_row['timestamp'] = record['timestamp']\n",
    "                    weather_data.append(weather_row)\n",
    "                    continue\n",
    "            \n",
    "            # If no match found or no weather data, add empty record\n",
    "            weather_data.append({\n",
    "                'activity_id': record['activity_id'],\n",
    "                'timestamp': record['timestamp'],\n",
    "                **{param: None for param in hourly_params}\n",
    "            })\n",
    "    \n",
    "    weather_df = pd.DataFrame(weather_data)\n",
    "    \n",
    "    # Merge weather data with original data\n",
    "    result = pd.merge(\n",
    "        df.drop(['lat_rounded', 'lon_rounded', 'weather_timestamp'], axis=1),\n",
    "        weather_df,\n",
    "        on=['activity_id', 'timestamp'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Save to new CSV\n",
    "    result.to_csv(output_path, index=False)\n",
    "    print(f\"Saved enriched data to {output_path}\")\n",
    "    \n",
    "    # Print some stats\n",
    "    print(\"\\nWeather data statistics:\")\n",
    "    for param in hourly_params:\n",
    "        missing = result[param].isna().sum()\n",
    "        print(f\"{param}: {missing} missing values ({missing/len(result)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Found 3395 unique location-time combinations\n",
      "Fetching weather data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [29:21<00:00, 51.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding weather data to records...\n",
      "Saved enriched data to ./data/records_with_weather.csv\n",
      "\n",
      "Weather data statistics:\n",
      "temperature_2m: 0 missing values (0.0%)\n",
      "relative_humidity_2m: 0 missing values (0.0%)\n",
      "dew_point_2m: 0 missing values (0.0%)\n",
      "wind_speed_10m: 0 missing values (0.0%)\n",
      "wind_direction_10m: 0 missing values (0.0%)\n",
      "precipitation: 0 missing values (0.0%)\n",
      "cloud_cover: 0 missing values (0.0%)\n",
      "surface_pressure: 0 missing values (0.0%)\n"
     ]
    }
   ],
   "source": [
    "add_weather_data(\n",
    "    input_path='./data/records_converted.csv',\n",
    "    output_path='./data/records_with_weather.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "\n",
    "- [ ] Add time into activity\n",
    "- [ ] Strip Garmin running power\n",
    "- [ ] Make sure Stryd running power is used\n",
    "- [ ] Calculate GAP for running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient statistics for running activities:\n",
      "count    2.352809e+06\n",
      "mean     1.654370e-03\n",
      "std      1.069148e-01\n",
      "min     -2.401199e+01\n",
      "25%     -1.311229e-02\n",
      "50%      0.000000e+00\n",
      "75%      1.451042e-02\n",
      "max      2.330814e+01\n",
      "Name: gradient, dtype: float64\n",
      "\n",
      "Grade adjusted speed vs original speed (running only):\n",
      "       original_speed  grade_adjusted_speed\n",
      "count    2.352809e+06          2.352809e+06\n",
      "mean     2.941069e+00          2.895937e+00\n",
      "std      5.264921e-01          5.910295e-01\n",
      "min      0.000000e+00          0.000000e+00\n",
      "25%      2.731000e+00          2.687000e+00\n",
      "50%      2.967000e+00          2.948000e+00\n",
      "75%      3.182000e+00          3.181356e+00\n",
      "max      2.185200e+01          2.185200e+01\n"
     ]
    }
   ],
   "source": [
    "def calculate_grade_adjusted_speed(df: pd.DataFrame, window_size: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate grade adjusted speed using a rolling window to determine gradient.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with latitude, longitude, altitude, and speed columns\n",
    "        window_size: Number of records to look forward/backward for gradient calculation\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_gradient(group):\n",
    "        \"\"\"Calculate gradient for each point using surrounding records\"\"\"\n",
    "        \n",
    "        # Convert lat/lon to distances\n",
    "        R = 6371000  # Earth radius in meters\n",
    "        \n",
    "        # Convert latitude and longitude to radians\n",
    "        lat = np.radians(group['latitude'])\n",
    "        lon = np.radians(group['longitude'])\n",
    "        \n",
    "        # Calculate distances between consecutive points\n",
    "        dlat = lat.diff()\n",
    "        dlon = dlon = lon.diff()\n",
    "        \n",
    "        # Haversine formula for distance\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat) * np.cos(lat.shift()) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        distances = R * c\n",
    "        \n",
    "        # Calculate elevation changes\n",
    "        elevation_changes = group['altitude'].diff()\n",
    "        \n",
    "        # Calculate gradients\n",
    "        gradients = elevation_changes / distances\n",
    "        \n",
    "        # Replace inf/nan with 0\n",
    "        gradients = gradients.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        # Use rolling average to smooth gradients\n",
    "        gradients = gradients.rolling(window=window_size, center=True).mean().fillna(0)\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def calculate_relative_cost(gradient):\n",
    "        \"\"\"Calculate relative cost using the formula\"\"\"\n",
    "        i = gradient * 100  # Convert to percentage\n",
    "        return 15.14 * (i/100)**2 - 2.896 * (i/100)\n",
    "    \n",
    "    # Group by activity_id to ensure we don't calculate gradients across activities\n",
    "    result = []\n",
    "    for activity_id, group in df.groupby('activity_id'):\n",
    "        if group['sport'].iloc[0] != 'running':\n",
    "            # Skip non-running activities\n",
    "            group['gradient'] = 0\n",
    "            group['grade_adjusted_speed'] = group['speed']\n",
    "            result.append(group)\n",
    "            continue\n",
    "            \n",
    "        # Calculate gradients\n",
    "        group = group.copy()\n",
    "        group['gradient'] = calculate_gradient(group)\n",
    "        \n",
    "        # Calculate relative cost\n",
    "        group['relative_cost'] = calculate_relative_cost(group['gradient'])\n",
    "        \n",
    "        # Calculate grade adjusted speed\n",
    "        group['grade_adjusted_speed'] = group['speed'] / (1 + group['relative_cost'])\n",
    "        \n",
    "        result.append(group)\n",
    "    \n",
    "    return pd.concat(result)\n",
    "\n",
    "df = pd.read_csv('data/records_with_weather.csv')\n",
    "df = calculate_grade_adjusted_speed(df)\n",
    "df.to_csv('data/records_with_grade.csv', index=False)\n",
    "\n",
    "# Print some statistics\n",
    "print(\"\\nGradient statistics for running activities:\")\n",
    "running_data = df[df['sport'] == 'running']\n",
    "print(running_data['gradient'].describe())\n",
    "print(\"\\nGrade adjusted speed vs original speed (running only):\")\n",
    "print(pd.DataFrame({\n",
    "    'original_speed': running_data['speed'],\n",
    "    'grade_adjusted_speed': running_data['grade_adjusted_speed']\n",
    "}).describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
